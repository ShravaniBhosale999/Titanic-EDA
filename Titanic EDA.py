# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ULZGdt-4feyN5jrLoJfrvn1X5uk1NLx2

#Titanic EDA

###Dataset: https://www.kaggle.com/shuofxz/titanic-machine-learning-from-disaster/tasks?taskId=2692

### 1. Importing Necessary Libraries
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

## to remove warnings
import warnings
warnings.filterwarnings("ignore")

"""### 2. Reading the Data"""

from google.colab import files

# Upload the files from your local system
uploaded = files.upload()

df_test = pd.read_csv("test.csv")

df_train = pd.read_csv("train.csv")

"""### 3. Exploratory Data Analysis"""

df_train

df_train.head()  #having a look on train data

df_train.info()

df_train.shape

# Checking NULL Values

df_train.isnull()

df_train.isnull().sum()

"""Observations:

~ We have Null values in Age, Cabin and embarked column. ~ We we will visualize this null data for better understanding.
"""

## we will use heatmap to visualize null data

sns.heatmap(df_train.isnull(), yticklabels=False,cmap="viridis")

"""Observations:

~ we get better understanding of null values in dataset.

###4. Visualizing the Data
"""

# making countplot to visualize the number of passengers survive
# Set Seaborn style
sns.set_style("whitegrid")

# Explicitly specify 'x' for the column name
sns.countplot(x="Survived", data=df_train, palette="rainbow")

# get the number of passenger survived

df_train['Survived'].value_counts()

"""Observation:

~ Less number of passenger haven't survived.
"""

# now we will see survival of females and males

sns.countplot(x="Survived",hue='Sex',palette='viridis',data=df_train)

"""Observation:

~ Less number of male passenger survived. ~ More number of female are survived.
"""

## We will see survival count respect to passenger class

sns.countplot(x="Survived",data=df_train,hue='Pclass',palette='rainbow')

"""Observation:

~ Less number of people survived who were in PClass 3. ~ More number of people survived who were in PClass 1.
"""

# Now we will plot the graph to get the count of male and female passengers in each class
sns.countplot(x='Pclass', data=df_train, palette="mako", hue="Sex")

"""Observations:

~ Pclass 1 and Pclass 2 has less number of male and female as compared to Pclass 3.
"""

# Countplot for Sibling and spouse

sns.countplot(x="SibSp",data=df_train,palette='rocket')

"""Observations:

~ Most of the passenger were travelling alone. ~ Average passengers were travlling with 1 spouse or siblings.
"""

# Count plot for suvival and sibsp

sns.countplot(x='Survived',data=df_train,palette="mako",hue="SibSp")

#Countplot for age and sex

sns.countplot(x='Age',data=df_train,palette='rocket',hue='Sex')

# Using pairplot for visibility of all data

sns.pairplot(df_train)

##plotting histogram to get maximum number oge (After dropping age)

sns.distplot(df_train['Age'].dropna(),kde=False,color='b',bins=40)

"""Observations:

~ Most of the passengers age lies between 20-35
"""

#we will find average fare of the passengers using histogram

sns.distplot(df_train['Fare'],kde=False,color='b',bins=30)

#Joint plot for survival and Pclass paramter to see corelation

sns.jointplot(data=df_train,x='Survived',y='Pclass',kind='kde',color='seagreen')

sns.jointplot(data=df_train,x='Pclass',y='Age',color="purple",kind='kde')

## Plotting box plot to get more information

sns.boxplot(data=df_train,x="Survived",y="Age",palette='rainbow')

"""Observation:

~ Survived age of people was average 29
"""

## Boxplot to get average age wrt to sex

sns.boxplot(data=df_train,y="Age",x="Sex",palette="mako")

"""Observation:

~ Average age of male was 26-29 ~ Average age of female was 23-27
"""

## Boxplot to get average age wrt pclass

sns.boxplot(data=df_train,x="Pclass",y="Age",palette="magma")

"""Observation:

~ Average age of in Pclass 1 is 38 ~ Average age of in Pclass 2 is 29 ~ Average age of in Pclass 3 is 24

## 5. Data Cleaning
"""

df_train_numeric = df_train.select_dtypes(include=[np.number])
df_train_numeric.corr()

sns.heatmap(df_train_numeric.corr())

sns.heatmap(df_train.isnull(),cmap='viridis',cbar=False,yticklabels=False)

def impute_age(cols):
    Age = cols[0]
    Pclass = cols[1]

   #if age column value is null
    if pd.isnull(Age):

       ## Pclass is 1 ,replace age to 37
        if Pclass == 1:
            return 38

        ## Pclass is 2 ,replace age to 29
        elif Pclass == 2:
            return 29

        ## Pclass is 3 ,replace age to 24
        else:
            return 24

    else:
        return Age

df_train['Age']=df_train[['Age','Pclass']].apply(impute_age,axis=1)  ## applying

sns.heatmap(df_train.isnull(),cmap='viridis',cbar=False,yticklabels=False)

## Now we will drop cabin column

df_train.drop('Cabin',axis=1,inplace=True)

sns.heatmap(df_train.isnull(),cmap='viridis',cbar=False,yticklabels=False)

# Finally we remove some remaining null values

df_train.dropna(inplace=True)

sns.heatmap(df_train.isnull(),cmap='viridis',cbar=False,yticklabels=False)

df_train.isnull().sum()

"""So now, we dont have NULL values in our dataset"""

df_train.head()

"""##6. Converting Catagorical Columns

We will convert embarked and sex column to understand those features to ML algorithm
"""

sex=pd.get_dummies(df_train['Sex'],drop_first=True)
Embark=pd.get_dummies(df_train['Embarked'],drop_first=True)

sex.head()

Embark.head()

# We will now remove unwanted columns which are not required in dataset

df_train.drop(['Name','Ticket','Embarked','Sex'],inplace=True,axis=1)

df_train.head()

# Now we will add the converted columns to the data

df_train = pd.concat([df_train,sex,Embark],axis=1)

df_train.head()

"""Now our dataset is ready to model

## 7. Train Test Slipt
"""

x= df_train.drop('Survived',axis=1)  ## Independent Variables
x.head()

y=df_train['Survived']  ## Dependent Variable
y.head()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(x,y,test_size = 0.40,random_state=1231)

"""##8. Logistic Regression"""

## We will import logistic regression and train the model

from sklearn.linear_model import LogisticRegression

# Create an instance of the Logistic Regression model
logmodel = LogisticRegression()

# Display the model's parameters (hyperparameters)
print(logmodel.get_params())

## lets see the predictions

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

logmodel = LogisticRegression()

logmodel.fit(X_train, y_train)

predictions = logmodel.predict(X_test)

# Display the classification report
print(classification_report(y_test, predictions))

## lets see the confusion matrix

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, predictions)

"""True Positive: 107 True Negative: 92

False Positive: 29 False Negative: 43
"""

## Accuracy score

from sklearn.metrics import accuracy_score
accuracy=accuracy_score(y_test,predictions)
accuracy

"""# Hence, the accuracy score was 0.79 ,so around 79% of passengers survived in Titanic and 21% did not survived"""

